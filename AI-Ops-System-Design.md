# AI 智能运维助手架构设计文档

## 1. 项目目标

本项目旨在设计并构建一个先进、可扩展且安全的AI智能运维助手。该助手能够理解用户的自然语言指令，将其转化为精确的、结构化的运维意图，并最终安全地执行相应的系统命令。

核心目标是摆脱将运维知识硬编码在Prompt中的初级模式，转向一个基于 **检索增强生成 (Retrieval-Augmented Generation, RAG)** 的企业级架构。

---

## 2. 核心架构：检索增强生成 (RAG)

我们采用RAG作为系统的核心架构，以实现知识库与大语言模型（LLM）的解耦。

*   **传统方式的问题**：将所有服务和命令硬编码在Prompt中，导致Prompt冗长、难以维护、扩展性差，且无法处理动态参数（如服务器IP）。
*   **RAG的优势**：
    *   **高扩展性**：知识库可以无限扩展，而无需修改核心Prompt。
    *   **高准确性**：AI的回答基于我们提供的、可信的知识库，而不是凭空“幻觉”。
    *   **高灵活性**：能够轻松处理动态、变化的运维实体，如服务器、文件名等。
    *   **易于维护**：运维知识以结构化方式存储在外部，增删改查非常方便。

**工作流程概览**:

```mermaid
graph TD
    A[用户输入] --> B{1. 检索模块};
    B -- "查询" --> C[外部知识库 / 向量数据库];
    C -- "返回相关知识" --> B;
    B --> D{2. Prompt 组装模块};
    A --> D;
    D -- "生成增强后的 Prompt" --> E[大语言模型(LLM)];
    E -- "生成结构化意图 (JSON)" --> F[后端应用];
    F -- "安全组装命令" --> G[执行命令];
```

---

## 3. 架构组件详解

### 3.1. 外部知识库 (External Knowledge Base)

这是系统的“大脑”，存储所有运维知识。关键在于使用 **命令模板** 和 **参数化** 的思想。

*   **存储内容**:
    *   **操作能力 (Action/Ability)**: 定义系统能做什么，如 `检查服务状态`、`传输文件`。
    *   **意图标识 (Intent ID)**: 每个能力的唯一ID，如 `server:check_status`。
    *   **命令模板 (Command Template)**: 带有占位符的命令，如 `ssh ${user}@${server_ip} 'systemctl status ${service_name}'`。
    *   **所需参数 (Required Parameters)**: 模板中需要填充的参数列表，如 `["user", "server_ip", "service_name"]`。
    *   **触发词/描述 (Keywords/Description)**: 用于语义检索的文本，如“检查服务运行情况、服务状态、怎么样了、挂了没”。

*   **存储形式**: 可以是JSON/YAML文件，或更专业的数据库/向量数据库。

### 3.2. 检索模块 (Retrieval Module)

该模块负责从用户输入中理解其意图，并从知识库中找到最相关的“操作能力”。

*   **核心技术**: **语义搜索/向量搜索**。
*   **准备阶段 (Offline)**:
    1.  选择一个 **Embedding模型** (如 Sentence-Transformers)。
    2.  将知识库中所有“操作能力”的描述文本转换为向量。
    3.  将文本及其对应的向量存入 **向量数据库** (如 ChromaDB, FAISS)。
*   **检索阶段 (Online)**:
    1.  将用户的输入（如“帮我看看支付服务在10.0.0.5上是不是挂了”）也转换为向量。
    2.  在向量数据库中进行相似度搜索，找到最匹配的“操作能力”（如 `server:check_status`）。

### 3.3. 生成模块 (Generation Module)

该模块的核心是 **大语言模型 (LLM)**，但它的任务不是生成命令，而是生成结构化的意图数据。

*   **动态Prompt**: 后端程序会动态组装一个Prompt，包含：
    1.  **系统指令**: "你是一个AI助手，请根据上下文和用户问题，提取意图和参数，并输出JSON。"
    2.  **上下文 (Context)**: 从检索模块获取到的最相关的知识片段。
    3.  **用户问题 (Query)**: 用户的原始输入。
*   **AI的任务**:
    1.  **意图确认**: 根据上下文，确认用户的意图是 `server:check_status`。
    2.  **实体提取**: 从用户问题中提取出模板所需的参数，如 `service_name: "支付服务"` 和 `server_ip: "10.0.0.5"`。
*   **输出**: 生成一个标准的JSON对象，绝不包含最终命令。
    ```json
    {
      "intent": "server:check_status",
      "parameters": {
        "service_name": "支付服务",
        "server_ip": "10.0.0.5"
      },
      "riskLevel": "low", 
      "isCommand": true
    }
    ```

---

## 4. 优化与质量控制

为确保检索的准确性，需要对 **相关度分数** 进行控制。

*   **策略**: 推荐使用 **“Top-K + 阈值”** 的混合策略。
    1.  **检索Top-K**: 总是先检索出最相似的K个结果（如K=3）。
    2.  **阈值过滤**: 丢弃分数低于预设阈值（如0.8）的结果。
    3.  **空结果处理**: 如果过滤后没有剩下任何结果，则礼貌地回复用户“无法理解或知识库暂无此项能力”。
*   **阈值确定**: 阈值没有固定值，需通过对一小批业务数据进行 **经验性测试** 来调优。一般可以从 **0.75-0.85** (余弦相似度) 开始尝试。

---

## 5. 安全考量

**安全是本架构的最高优先级之一。**

*   **核心原则**: AI（LLM）永远不接触、不生成最终的可执行命令。
*   **责任分离**:
    *   **AI**: 负责“理解”和“信息提取”，输出结构化数据。
    *   **后端应用**: 负责“验证”、“组装”和“执行”。
*   **执行流程**: 后端应用接收到AI的JSON输出后，必须由后端代码根据`intent`查找命令模板，并使用`parameters`安全地填充模板，最后才执行由自己亲手构建的命令。这从根本上杜绝了Prompt注入攻击的风险。

---
这份文档为您构建一个强大且可扩展的AI运维助手提供了全面的指导。通过遵循RAG架构并坚持安全责任分离的原则，最终的系统将是功能强大、灵活且企业级的。